# -*- coding: utf-8 -*-
"""Midsem_Report_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kJCnhZjh5Trz4lFZ4teAaho9Q1o43mew
"""

# !pip install transformers
# !pip install sentencepiece
# !pip install py-rouge
# !pip install rouge_score

# """#Download dataset n extract"""

# !wget https://zenodo.org/records/7152317/files/dataset.zip?download=1
# !unzip '/content/dataset.zip?download=1'

"""## Preprocess the data into dataframe"""

import os
import random
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
def prepare_val_list():
    val_files = []  # Add the validation files to be used
    for i in range(1, 50):
        x = random.randint(1, 7028)
        if x not in val_files:
            val_files.append(x)

    val_files = list(map(lambda x: str(x) + '.txt', val_files))
    return val_files

def getData(model_name, dataPath, MAX_DOC_LEN, val=0):
    model_name = "nsi319/legal-led-base-16384"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    documentPath = f'{dataPath}/judgement'
    summaryPath = f'{dataPath}/summary'
    dataset = {'document': [], 'summary': []}
    count = 0
    for file in os.listdir(documentPath):
        count += 1
        if os.stat(f'{documentPath}/{file}').st_size == 0 or os.stat(f'{summaryPath}/{file}').st_size == 0:
            continue
        doc_in = open(f'{documentPath}/{file}', 'r', encoding='utf8')
        doc_lines = [line.strip() for line in doc_in.readlines()]
        summ_in = open(f'{summaryPath}/{file}', 'r', encoding='utf8')
        summ_lines = [line.strip() for line in summ_in.readlines()]
        if len(doc_lines) == 0 or len(summ_lines) == 0:
            continue

        if val == 0 and file not in val_files:
            dataset['document'].append(' '.join(doc_lines))
            dataset['summary'].append(' '.join(summ_lines))
        if val == 1 and file in val_files:
            dataset['document'].append(' '.join(doc_lines))
            dataset['summary'].append(' '.join(summ_lines))

    df = pd.DataFrame(dataset)
    return df

model_name = "nsi319/legal-led-base-16384"
tokenizer = AutoTokenizer.from_pretrained(model_name)
MAXLENGTH = 6000
dataPath = "/content/dataset/IN-Abs"
val_files = prepare_val_list()
train_df = getData(tokenizer, f'{dataPath}/train-data', MAXLENGTH)
val_df = getData(tokenizer, f'{dataPath}/train-data', MAXLENGTH, 1)
train_df

train_docs = train_df['document'].values.tolist()[:50]
train_sums = train_df['summary'].values.tolist()[:50]

"""
# Finetune/ Custom model for LEGAL LED"""

batch_size = 2
learning_rate = 1e-3
num_epochs = 3

"""## TOKENISE X N Y"""

model_name = "nsi319/legal-led-base-16384"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained("nsi319/legal-led-base-16384")

train_encodings = tokenizer(train_docs, train_sums, truncation=True, padding=True, return_tensors="pt")
# Prepare labels for training
labels = tokenizer(train_sums, truncation=True, padding=True, return_tensors="pt")["input_ids"]

"""## Train custom model"""

from torch.utils.data import DataLoader, TensorDataset

train_dataset = TensorDataset(train_encodings["input_ids"], train_encodings["attention_mask"], labels)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

import torch

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    model.train()
    for batch in train_loader:
        input_ids, attention_mask, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item()}")

model.save_pretrained("fine_tuned_model")
tokenizer.save_pretrained("fine_tuned_model")

input_text = "Your input document goes here."
input_encoding = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True)
output_ids = model.generate(**input_encoding)
output_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print("Generated Summary:", output_summary)

"""# ROUGE SCORE"""

from rouge_score import rouge_scorer

def rouge_score(references, predictions):
  print(references)
  print(predictions)

  # Initialize the Rouge Scorer
  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

  # Calculate ROUGE scores
  scores = scorer.score(predictions, references)

  # Print ROUGE scores
  for metric, score in scores.items():
      print(f"{metric}: {score.fmeasure:.4f}")

rouge_score("This is the summary of the given text", "This  is the summary of given ")

